{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from retrieval_models import ModernNCA\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset to keep track of indices\n",
    "class IndexedTensorDataset(Dataset):\n",
    "    def __init__(self, tensors_num, tensors_cat, targets):\n",
    "        self.tensors_num = tensors_num\n",
    "        self.tensors_cat = tensors_cat\n",
    "        self.targets = targets\n",
    "        self.indices = torch.arange(len(tensors_num), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.tensors_num[index], self.tensors_cat[index], self.targets[index], self.indices[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors_num)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def label_encode(X, categorical_columns):\n",
    "    le_categorical = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        le_categorical[col] = le  # Save each encoder if needed for inverse transformation later\n",
    "    return X, le_categorical\n",
    "\n",
    "def llm_encoder(X, categorical_columns):\n",
    "    embeddings = []\n",
    "    for idx in range(X.shape[0]):\n",
    "        string = ''\n",
    "        for column in categorical_columns:\n",
    "            string += f\"{column}: {X[column][idx]}. \"\n",
    "        embeddings.append(string)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "    encoded_input = tokenizer(embeddings, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**encoded_input)\n",
    "    embeddings = mean_pooling(embeddings, encoded_input['attention_mask'])\n",
    "    embeddings = pd.DataFrame(embeddings.numpy(), columns=[f'embedding_{i}' for i in range(embeddings.shape[1])])\n",
    "    X = pd.concat([X, embeddings], axis=1)\n",
    "    X = X.drop(columns=categorical_columns)\n",
    "    return X\n",
    "\n",
    "def onehot_encode(X, categorical_columns):\n",
    "    X = pd.get_dummies(X, columns=categorical_columns, drop_first = True)\n",
    "    return X\n",
    "\n",
    "def load_dataset(config):\n",
    "    # Fetch the dataset\n",
    "    data = fetch_openml(config['dataset']['name'], version=config['dataset']['version'], as_frame=True)\n",
    "    X = data['data'].copy()\n",
    "    y = data['target']\n",
    "\n",
    "    # Downsample the dataset\n",
    "    X = X[:config['dataset']['n_samples']]\n",
    "    y = y[:config['dataset']['n_samples']]\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_columns = X.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    if config['dataset']['cat_encode'] == 'label':\n",
    "        X, _ = label_encode(X, categorical_columns)\n",
    "    elif config['dataset']['cat_encode'] == 'onehot':\n",
    "        X = onehot_encode(X, categorical_columns)\n",
    "    elif config['dataset']['cat_encode'] == 'llm':\n",
    "        X = llm_encoder(X, categorical_columns)\n",
    "    \n",
    "    # Scale numerical columns\n",
    "    numerical_transformer = StandardScaler()\n",
    "    X[numerical_columns] = numerical_transformer.fit_transform(X[numerical_columns])\n",
    "\n",
    "    # Encode the target variable\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y)\n",
    "\n",
    "    # Convert X and y to tensor\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    if config['dataset']['all_num']:\n",
    "        X_num = torch.tensor(X.values, dtype=torch.float32)\n",
    "        X_cat = X_num.clone()\n",
    "        d_in_num = X_num.shape[1]\n",
    "        d_in_cat = 0\n",
    "    else:\n",
    "        X_num = torch.tensor(X[numerical_columns].values, dtype=torch.float32)\n",
    "        X_cat = torch.tensor(X.drop(columns=numerical_columns).values, dtype=torch.float32)\n",
    "        d_in_num = X_num.shape[1]\n",
    "        d_in_cat = X_cat.shape[1]\n",
    "\n",
    "    return (X_num, X_cat, y, d_in_num, d_in_cat, le_target)\n",
    "\n",
    "\n",
    "def evaluate(config, model, test_loader, criterion, device, X_num_train, X_cat_train, y_train):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_num_batch, X_cat_batch, y_batch, idx_batch in test_loader:\n",
    "            X_num_batch = X_num_batch.to(device)\n",
    "            X_cat_batch = X_cat_batch.to(device) if not config['dataset']['all_num'] else None\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Use entire training data as candidates during evaluation\n",
    "            candidate_x_num = X_num_train.to(device) \n",
    "            candidate_x_cat = X_cat_train.to(device) if not config['dataset']['all_num'] else None\n",
    "            candidate_y = y_train.to(device)\n",
    "\n",
    "            # Forward pass with separate categorical and numerical features\n",
    "            logits = model(\n",
    "                x_num=X_num_batch,\n",
    "                x_cat=X_cat_batch,\n",
    "                y=None,\n",
    "                candidate_x_num=candidate_x_num,\n",
    "                candidate_x_cat=candidate_x_cat,\n",
    "                candidate_y=candidate_y,\n",
    "                is_train=False\n",
    "            )\n",
    "\n",
    "            # Convert logits to predictions\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(logits, y_batch)\n",
    "            test_loss += loss.item() * y_batch.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    test_loss = test_loss / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "def train(config):\n",
    "    set_seed(config['dataset']['random_state'])\n",
    "\n",
    "    X_num, X_cat, y, d_in_num, d_in_cat, le_target = load_dataset(config)\n",
    "    output_classes = len(le_target.classes_)\n",
    "\n",
    "    X_num_train, X_num_test, X_cat_train, X_cat_test, y_train, y_test = train_test_split(\n",
    "        X_num, X_cat, y, test_size=config['dataset']['test_size'], random_state=config['dataset']['random_state']\n",
    "    )\n",
    "\n",
    "    # Use IndexedTensorDataset for separate numerical and categorical data\n",
    "    train_dataset = IndexedTensorDataset(X_num_train, X_cat_train, y_train)\n",
    "    test_dataset = IndexedTensorDataset(X_num_test, X_cat_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config['dataset']['batch_size'], shuffle=True,\n",
    "        pin_memory=True, num_workers=4\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=config['dataset']['batch_size'], shuffle=False,\n",
    "        pin_memory=True, num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize the ModernNCA model\n",
    "    model = ModernNCA(\n",
    "        d_in_num=d_in_num,\n",
    "        d_in_cat=d_in_cat,\n",
    "        d_out=output_classes,\n",
    "        dim=config['model']['dim'],\n",
    "        dropout=config['model']['dropout'],\n",
    "        n_frequencies=config['model']['n_frequencies'],\n",
    "        frequency_scale=config['model']['frequency_scale'],\n",
    "        d_embedding=config['model']['d_embedding'],\n",
    "        lite=config['model']['lite'],\n",
    "        temperature=config['model']['temperature'],\n",
    "        sample_rate=config['model']['sample_rate'],\n",
    "        use_llama=config['model']['use_llama'],\n",
    "        llama_model_name=config['model']['llama_model_name'],\n",
    "        start_layer=config['model']['start_layer'],\n",
    "        end_layer=config['model']['end_layer']\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=config['training']['learning_rate'],\n",
    "        weight_decay=config['training']['weight_decay']\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config['training']['epochs']):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for itr, (X_num_batch, X_cat_batch, y_batch, idx_batch) in enumerate(train_loader):\n",
    "            X_num_batch = X_num_batch.to(device)\n",
    "            X_cat_batch = X_cat_batch.to(device) if not config['dataset']['all_num'] else None\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Exclude current batch for candidates\n",
    "            mask = torch.isin(torch.arange(X_num_train.shape[0]), idx_batch)\n",
    "            candidate_x_num = X_num_train[~mask].to(device)\n",
    "            candidate_x_cat = X_cat_train[~mask].to(device) if not config['dataset']['all_num'] else None\n",
    "            candidate_y = y_train[~mask].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with separate categorical and numerical features\n",
    "            logits = model(\n",
    "                x_num=X_num_batch,\n",
    "                x_cat=X_cat_batch,\n",
    "                y=y_batch,\n",
    "                candidate_x_num=candidate_x_num,\n",
    "                candidate_x_cat=candidate_x_cat,\n",
    "                candidate_y=candidate_y,\n",
    "                is_train=True\n",
    "            )\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * y_batch.size(0)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "            if itr % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{config[\"training\"][\"epochs\"]}]: Batch [{itr+1}/{len(train_loader)}] | Accuracy: {correct/total:.2f}')\n",
    "\n",
    "        epoch_loss = epoch_loss / total\n",
    "        epoch_acc = 100 * correct / total\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{config[\"training\"][\"epochs\"]}] | Loss: {epoch_loss:.4f} | '\n",
    "                f'Accuracy: {epoch_acc:.2f}% | Time: {epoch_time:.2f}s')\n",
    "\n",
    "    evaluate(config, model, test_loader, criterion, device, X_num_train, X_cat_train, y_train)\n",
    "\n",
    "    # save the model\n",
    "    path = os.getcwd() + '/TabLLM/initial_exp/' + config['training']['model_path']\n",
    "    torch.save(model.state_dict(), path)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    evaluate(config, model, test_loader, criterion, device, X_num_train, X_cat_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(os.getcwd() + '/TabLLM/initial_exp/retrieval_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: Batch [1/250] | Accuracy: 0.81\n",
      "Epoch [1/10]: Batch [51/250] | Accuracy: 0.83\n",
      "Epoch [1/10]: Batch [101/250] | Accuracy: 0.84\n",
      "Epoch [1/10]: Batch [151/250] | Accuracy: 0.84\n",
      "Epoch [1/10]: Batch [201/250] | Accuracy: 0.84\n",
      "Epoch [1/10] | Loss: 0.3278 | Accuracy: 84.36% | Time: 54.68s\n",
      "Epoch [2/10]: Batch [1/250] | Accuracy: 0.84\n",
      "Epoch [2/10]: Batch [51/250] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [101/250] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [151/250] | Accuracy: 0.85\n",
      "Epoch [2/10]: Batch [201/250] | Accuracy: 0.86\n",
      "Epoch [2/10] | Loss: 0.3104 | Accuracy: 85.54% | Time: 54.19s\n",
      "Epoch [3/10]: Batch [1/250] | Accuracy: 0.69\n",
      "Epoch [3/10]: Batch [51/250] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [101/250] | Accuracy: 0.85\n",
      "Epoch [3/10]: Batch [151/250] | Accuracy: 0.85\n",
      "Epoch [3/10]: Batch [201/250] | Accuracy: 0.85\n",
      "Epoch [3/10] | Loss: 0.3076 | Accuracy: 85.49% | Time: 54.35s\n",
      "Epoch [4/10]: Batch [1/250] | Accuracy: 0.88\n",
      "Epoch [4/10]: Batch [51/250] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [101/250] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [151/250] | Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
