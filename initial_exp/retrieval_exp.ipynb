{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "os.chdir('/home/mrsergazinov/TabLLM/initial_exp/')\n",
    "from retrieval_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset to keep track of indices\n",
    "class IndexedTensorDataset(Dataset):\n",
    "    def __init__(self, tensors_num, tensors_cat, targets):\n",
    "        self.tensors_num = tensors_num\n",
    "        self.tensors_cat = tensors_cat\n",
    "        self.targets = targets\n",
    "        self.indices = torch.arange(len(tensors_num), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.tensors_num[index], self.tensors_cat[index], self.targets[index], self.indices[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors_num)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def label_encode(X, categorical_columns):\n",
    "    le_categorical = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        le_categorical[col] = le  # Save each encoder if needed for inverse transformation later\n",
    "    return X, le_categorical\n",
    "\n",
    "def llm_encoder(X, categorical_columns, batch_size=256):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Step 1: Prepare text embeddings\n",
    "    embeddings = []\n",
    "    for idx in range(X.shape[0]):\n",
    "        string = ''\n",
    "        for column in categorical_columns:\n",
    "            string += f\"{column}: {X[column][idx]}. \"\n",
    "        embeddings.append(string)\n",
    "\n",
    "    # Load tokenizer and model, move model to device\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2').to(device)\n",
    "\n",
    "    # Step 2: Process embeddings in batches\n",
    "    batch_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(embeddings), batch_size):\n",
    "            batch_texts = embeddings[i:i+batch_size]\n",
    "            encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "            model_output = model(**encoded_input)\n",
    "            batch_embeddings.append(mean_pooling(model_output, encoded_input['attention_mask']).cpu())\n",
    "            print(f'Processed {i+batch_size}/{len(embeddings)} embeddings')\n",
    "    \n",
    "    # Concatenate all batch embeddings\n",
    "    embeddings_tensor = torch.cat(batch_embeddings, dim=0)\n",
    "    embeddings_df = pd.DataFrame(embeddings_tensor.numpy(), columns=[f'embedding_{i}' for i in range(embeddings_tensor.shape[1])])\n",
    "\n",
    "    # Clean up memory\n",
    "    del encoded_input, model_output, batch_embeddings, embeddings_tensor, tokenizer, model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Step 3: Concatenate embeddings with the original DataFrame and drop categorical columns\n",
    "    X = pd.concat([X.reset_index(drop=True), embeddings_df], axis=1)\n",
    "    X = X.drop(columns=categorical_columns)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def onehot_encode(X, categorical_columns):\n",
    "    X = pd.get_dummies(X, columns=categorical_columns, drop_first = True)\n",
    "    return X\n",
    "\n",
    "def load_dataset(config):\n",
    "    # Fetch the dataset\n",
    "    data = fetch_openml(config['dataset']['name'], version=config['dataset']['version'], as_frame=True)\n",
    "    X = data['data'].copy()\n",
    "    y = data['target']\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_columns = X.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    if config['dataset']['cat_encode'] == 'label':\n",
    "        X, _ = label_encode(X, categorical_columns)\n",
    "    elif config['dataset']['cat_encode'] == 'onehot':\n",
    "        X = onehot_encode(X, categorical_columns)\n",
    "    elif config['dataset']['cat_encode'] == 'llm':\n",
    "        X = llm_encoder(X, categorical_columns)\n",
    "    \n",
    "    # Scale numerical columns\n",
    "    numerical_transformer = StandardScaler()\n",
    "    X[numerical_columns] = numerical_transformer.fit_transform(X[numerical_columns])\n",
    "\n",
    "    # Encode the target variable\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y)\n",
    "\n",
    "    # Convert X and y to tensor\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    if config['dataset']['all_num']:\n",
    "        X_num = torch.tensor(X.values, dtype=torch.float32)\n",
    "        X_cat = X_num.clone()\n",
    "        d_in_num = X_num.shape[1]\n",
    "        d_in_cat = 0\n",
    "    else:\n",
    "        X_num = torch.tensor(X[numerical_columns].values, dtype=torch.float32)\n",
    "        X_cat = torch.tensor(X.drop(columns=numerical_columns).values, dtype=torch.float32)\n",
    "        d_in_num = X_num.shape[1]\n",
    "        d_in_cat = X_cat.shape[1]\n",
    "\n",
    "    return (X_num, X_cat, y, d_in_num, d_in_cat, le_target)\n",
    "\n",
    "\n",
    "def evaluate(config, model, test_loader, criterion, device, X_num_train, X_cat_train, y_train):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_num_batch, X_cat_batch, y_batch, idx_batch in test_loader:\n",
    "            X_num_batch = X_num_batch.to(device)\n",
    "            X_cat_batch = X_cat_batch.to(device) if not config['dataset']['all_num'] else None\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Use entire training data as candidates during evaluation\n",
    "            candidate_x_num = X_num_train.to(device) \n",
    "            candidate_x_cat = X_cat_train.to(device) if not config['dataset']['all_num'] else None\n",
    "            candidate_y = y_train.to(device)\n",
    "\n",
    "            # Forward pass with separate categorical and numerical features\n",
    "            logits = model(\n",
    "                x_num=X_num_batch,\n",
    "                x_cat=X_cat_batch,\n",
    "                y=None,\n",
    "                candidate_x_num=candidate_x_num,\n",
    "                candidate_x_cat=candidate_x_cat,\n",
    "                candidate_y=candidate_y\n",
    "            )\n",
    "\n",
    "            # Convert logits to predictions\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(logits, y_batch)\n",
    "            test_loss += loss.item() * y_batch.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    test_loss = test_loss / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "def train(config):\n",
    "    set_seed(config['dataset']['random_state'])\n",
    "\n",
    "    X_num, X_cat, y, d_in_num, d_in_cat, le_target = load_dataset(config)\n",
    "    output_classes = len(le_target.classes_)\n",
    "\n",
    "    X_num_train, X_num_test, X_cat_train, X_cat_test, y_train, y_test = train_test_split(\n",
    "        X_num, X_cat, y, test_size=config['dataset']['test_size'], random_state=config['dataset']['random_state']\n",
    "    )\n",
    "\n",
    "    # Use IndexedTensorDataset for separate numerical and categorical data\n",
    "    train_dataset = IndexedTensorDataset(X_num_train, X_cat_train, y_train)\n",
    "    test_dataset = IndexedTensorDataset(X_num_test, X_cat_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config['dataset']['batch_size'], shuffle=True,\n",
    "        pin_memory=True, num_workers=4\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=config['dataset']['batch_size'], shuffle=False,\n",
    "        pin_memory=True, num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize the ModernNCA model\n",
    "    model = ModernNCA(\n",
    "        d_in_num=d_in_num,\n",
    "        d_in_cat=d_in_cat,\n",
    "        d_out=output_classes,\n",
    "        dim=config['model']['dim'],\n",
    "        dropout=config['model']['dropout'],\n",
    "        n_frequencies=config['model']['n_frequencies'],\n",
    "        frequency_scale=config['model']['frequency_scale'],\n",
    "        d_embedding=config['model']['d_embedding'],\n",
    "        lite=config['model']['lite'],\n",
    "        temperature=config['model']['temperature'],\n",
    "        sample_rate=config['model']['sample_rate'],\n",
    "        use_llama=config['model']['use_llama'],\n",
    "        llama_model_name=config['model']['llama_model_name'],\n",
    "        start_layer=config['model']['start_layer'],\n",
    "        end_layer=config['model']['end_layer']\n",
    "    )\n",
    "    model = model.float()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=config['training']['learning_rate'],\n",
    "        weight_decay=config['training']['weight_decay']\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config['training']['epochs']):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for itr, (X_num_batch, X_cat_batch, y_batch, idx_batch) in enumerate(train_loader):\n",
    "            X_num_batch = X_num_batch.to(device)\n",
    "            X_cat_batch = X_cat_batch.to(device) if not config['dataset']['all_num'] else None\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Exclude current batch for candidates\n",
    "            mask = ~torch.isin(torch.arange(X_num_train.shape[0]), idx_batch)\n",
    "            true_indices = torch.arange(X_num_train.shape[0])[mask]\n",
    "            num_samples = int(len(true_indices) * config['model']['sample_rate'])\n",
    "            sampled_indices = true_indices[torch.randperm(len(true_indices))[:num_samples]]\n",
    "            sampled_mask = torch.zeros_like(mask, dtype=torch.bool)\n",
    "            sampled_mask[sampled_indices] = True\n",
    "\n",
    "            # Use the new sampled_mask to filter out elements\n",
    "            candidate_x_num = X_num_train[sampled_mask].to(device)\n",
    "            candidate_x_cat = X_cat_train[sampled_mask].to(device) if not config['dataset']['all_num'] else None\n",
    "            candidate_y = y_train[sampled_mask].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass with separate categorical and numerical features\n",
    "            logits = model(\n",
    "                x_num=X_num_batch,\n",
    "                x_cat=X_cat_batch,\n",
    "                y=y_batch,\n",
    "                candidate_x_num=candidate_x_num,\n",
    "                candidate_x_cat=candidate_x_cat,\n",
    "                candidate_y=candidate_y\n",
    "            )\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * y_batch.size(0)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "            if itr % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{config[\"training\"][\"epochs\"]}]: Batch [{itr+1}/{len(train_loader)}] | Accuracy: {correct/total:.2f}')\n",
    "\n",
    "        epoch_loss = epoch_loss / total\n",
    "        epoch_acc = 100 * correct / total\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{config[\"training\"][\"epochs\"]}] | Loss: {epoch_loss:.4f} | '\n",
    "                f'Accuracy: {epoch_acc:.2f}% | Time: {epoch_time:.2f}s')\n",
    "\n",
    "    evaluate(config, model, test_loader, criterion, device, X_num_train, X_cat_train, y_train)\n",
    "\n",
    "    # save the model\n",
    "    path = config['training']['model_path']\n",
    "    torch.save(model.state_dict(), path)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    evaluate(config, model, test_loader, criterion, device, X_num_train, X_cat_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('retrieval_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: Batch [1/1222] | Accuracy: 0.81\n",
      "Epoch [1/10]: Batch [51/1222] | Accuracy: 0.81\n",
      "Epoch [1/10]: Batch [101/1222] | Accuracy: 0.83\n",
      "Epoch [1/10]: Batch [151/1222] | Accuracy: 0.83\n",
      "Epoch [1/10]: Batch [201/1222] | Accuracy: 0.83\n",
      "Epoch [1/10]: Batch [251/1222] | Accuracy: 0.84\n",
      "Epoch [1/10]: Batch [301/1222] | Accuracy: 0.84\n",
      "Epoch [1/10]: Batch [351/1222] | Accuracy: 0.84\n",
      "Epoch [1/10]: Batch [401/1222] | Accuracy: 0.84\n",
      "Epoch [1/10]: Batch [451/1222] | Accuracy: 0.84\n",
      "Epoch [1/10]: Batch [501/1222] | Accuracy: 0.84\n",
      "Epoch [1/10]: Batch [551/1222] | Accuracy: 0.84\n",
      "Epoch [1/10]: Batch [601/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [651/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [701/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [751/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [801/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [851/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [901/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [951/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [1001/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [1051/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [1101/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [1151/1222] | Accuracy: 0.85\n",
      "Epoch [1/10]: Batch [1201/1222] | Accuracy: 0.85\n",
      "Epoch [1/10] | Loss: 0.3181 | Accuracy: 85.09% | Time: 10.90s\n",
      "Epoch [2/10]: Batch [1/1222] | Accuracy: 0.94\n",
      "Epoch [2/10]: Batch [51/1222] | Accuracy: 0.85\n",
      "Epoch [2/10]: Batch [101/1222] | Accuracy: 0.84\n",
      "Epoch [2/10]: Batch [151/1222] | Accuracy: 0.85\n",
      "Epoch [2/10]: Batch [201/1222] | Accuracy: 0.85\n",
      "Epoch [2/10]: Batch [251/1222] | Accuracy: 0.85\n",
      "Epoch [2/10]: Batch [301/1222] | Accuracy: 0.85\n",
      "Epoch [2/10]: Batch [351/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [401/1222] | Accuracy: 0.85\n",
      "Epoch [2/10]: Batch [451/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [501/1222] | Accuracy: 0.85\n",
      "Epoch [2/10]: Batch [551/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [601/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [651/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [701/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [751/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [801/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [851/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [901/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [951/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [1001/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [1051/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [1101/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [1151/1222] | Accuracy: 0.86\n",
      "Epoch [2/10]: Batch [1201/1222] | Accuracy: 0.86\n",
      "Epoch [2/10] | Loss: 0.3015 | Accuracy: 85.91% | Time: 10.83s\n",
      "Epoch [3/10]: Batch [1/1222] | Accuracy: 0.84\n",
      "Epoch [3/10]: Batch [51/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [101/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [151/1222] | Accuracy: 0.87\n",
      "Epoch [3/10]: Batch [201/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [251/1222] | Accuracy: 0.87\n",
      "Epoch [3/10]: Batch [301/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [351/1222] | Accuracy: 0.87\n",
      "Epoch [3/10]: Batch [401/1222] | Accuracy: 0.87\n",
      "Epoch [3/10]: Batch [451/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [501/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [551/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [601/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [651/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [701/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [751/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [801/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [851/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [901/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [951/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [1001/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [1051/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [1101/1222] | Accuracy: 0.86\n",
      "Epoch [3/10]: Batch [1151/1222] | Accuracy: 0.87\n",
      "Epoch [3/10]: Batch [1201/1222] | Accuracy: 0.86\n",
      "Epoch [3/10] | Loss: 0.2947 | Accuracy: 86.46% | Time: 10.72s\n",
      "Epoch [4/10]: Batch [1/1222] | Accuracy: 0.91\n",
      "Epoch [4/10]: Batch [51/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [101/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [151/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [201/1222] | Accuracy: 0.87\n",
      "Epoch [4/10]: Batch [251/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [301/1222] | Accuracy: 0.87\n",
      "Epoch [4/10]: Batch [351/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [401/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [451/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [501/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [551/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [601/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [651/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [701/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [751/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [801/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [851/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [901/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [951/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [1001/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [1051/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [1101/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [1151/1222] | Accuracy: 0.86\n",
      "Epoch [4/10]: Batch [1201/1222] | Accuracy: 0.86\n",
      "Epoch [4/10] | Loss: 0.2948 | Accuracy: 86.34% | Time: 10.80s\n",
      "Epoch [5/10]: Batch [1/1222] | Accuracy: 0.91\n",
      "Epoch [5/10]: Batch [51/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [101/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [151/1222] | Accuracy: 0.87\n",
      "Epoch [5/10]: Batch [201/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [251/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [301/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [351/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [401/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [451/1222] | Accuracy: 0.87\n",
      "Epoch [5/10]: Batch [501/1222] | Accuracy: 0.87\n",
      "Epoch [5/10]: Batch [551/1222] | Accuracy: 0.87\n",
      "Epoch [5/10]: Batch [601/1222] | Accuracy: 0.87\n",
      "Epoch [5/10]: Batch [651/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [701/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [751/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [801/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [851/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [901/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [951/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [1001/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [1051/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [1101/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [1151/1222] | Accuracy: 0.86\n",
      "Epoch [5/10]: Batch [1201/1222] | Accuracy: 0.86\n",
      "Epoch [5/10] | Loss: 0.2930 | Accuracy: 86.42% | Time: 10.49s\n",
      "Epoch [6/10]: Batch [1/1222] | Accuracy: 0.81\n",
      "Epoch [6/10]: Batch [51/1222] | Accuracy: 0.87\n",
      "Epoch [6/10]: Batch [101/1222] | Accuracy: 0.87\n",
      "Epoch [6/10]: Batch [151/1222] | Accuracy: 0.87\n",
      "Epoch [6/10]: Batch [201/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [251/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [301/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [351/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [401/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [451/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [501/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [551/1222] | Accuracy: 0.87\n",
      "Epoch [6/10]: Batch [601/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [651/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [701/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [751/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [801/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [851/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [901/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [951/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [1001/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [1051/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [1101/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [1151/1222] | Accuracy: 0.86\n",
      "Epoch [6/10]: Batch [1201/1222] | Accuracy: 0.86\n",
      "Epoch [6/10] | Loss: 0.2931 | Accuracy: 86.32% | Time: 10.78s\n",
      "Epoch [7/10]: Batch [1/1222] | Accuracy: 0.91\n",
      "Epoch [7/10]: Batch [51/1222] | Accuracy: 0.87\n",
      "Epoch [7/10]: Batch [101/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [151/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [201/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [251/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [301/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [351/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [401/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [451/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [501/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [551/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [601/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [651/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [701/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [751/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [801/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [851/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [901/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [951/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [1001/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [1051/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [1101/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [1151/1222] | Accuracy: 0.86\n",
      "Epoch [7/10]: Batch [1201/1222] | Accuracy: 0.86\n",
      "Epoch [7/10] | Loss: 0.2934 | Accuracy: 86.41% | Time: 11.44s\n",
      "Epoch [8/10]: Batch [1/1222] | Accuracy: 0.94\n",
      "Epoch [8/10]: Batch [51/1222] | Accuracy: 0.86\n",
      "Epoch [8/10]: Batch [101/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [151/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [201/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [251/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [301/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [351/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [401/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [451/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [501/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [551/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [601/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [651/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [701/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [751/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [801/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [851/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [901/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [951/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [1001/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [1051/1222] | Accuracy: 0.87\n",
      "Epoch [8/10]: Batch [1101/1222] | Accuracy: 0.86\n",
      "Epoch [8/10]: Batch [1151/1222] | Accuracy: 0.86\n",
      "Epoch [8/10]: Batch [1201/1222] | Accuracy: 0.87\n",
      "Epoch [8/10] | Loss: 0.2918 | Accuracy: 86.52% | Time: 11.65s\n",
      "Epoch [9/10]: Batch [1/1222] | Accuracy: 0.88\n",
      "Epoch [9/10]: Batch [51/1222] | Accuracy: 0.88\n",
      "Epoch [9/10]: Batch [101/1222] | Accuracy: 0.88\n",
      "Epoch [9/10]: Batch [151/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [201/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [251/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [301/1222] | Accuracy: 0.86\n",
      "Epoch [9/10]: Batch [351/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [401/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [451/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [501/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [551/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [601/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [651/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [701/1222] | Accuracy: 0.86\n",
      "Epoch [9/10]: Batch [751/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [801/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [851/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [901/1222] | Accuracy: 0.86\n",
      "Epoch [9/10]: Batch [951/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [1001/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [1051/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [1101/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [1151/1222] | Accuracy: 0.87\n",
      "Epoch [9/10]: Batch [1201/1222] | Accuracy: 0.87\n",
      "Epoch [9/10] | Loss: 0.2915 | Accuracy: 86.58% | Time: 10.61s\n",
      "Epoch [10/10]: Batch [1/1222] | Accuracy: 0.84\n",
      "Epoch [10/10]: Batch [51/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [101/1222] | Accuracy: 0.87\n",
      "Epoch [10/10]: Batch [151/1222] | Accuracy: 0.87\n",
      "Epoch [10/10]: Batch [201/1222] | Accuracy: 0.87\n",
      "Epoch [10/10]: Batch [251/1222] | Accuracy: 0.87\n",
      "Epoch [10/10]: Batch [301/1222] | Accuracy: 0.87\n",
      "Epoch [10/10]: Batch [351/1222] | Accuracy: 0.87\n",
      "Epoch [10/10]: Batch [401/1222] | Accuracy: 0.87\n",
      "Epoch [10/10]: Batch [451/1222] | Accuracy: 0.87\n",
      "Epoch [10/10]: Batch [501/1222] | Accuracy: 0.87\n",
      "Epoch [10/10]: Batch [551/1222] | Accuracy: 0.87\n",
      "Epoch [10/10]: Batch [601/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [651/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [701/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [751/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [801/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [851/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [901/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [951/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [1001/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [1051/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [1101/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [1151/1222] | Accuracy: 0.86\n",
      "Epoch [10/10]: Batch [1201/1222] | Accuracy: 0.86\n",
      "Epoch [10/10] | Loss: 0.2922 | Accuracy: 86.41% | Time: 10.29s\n",
      "Test Loss: 0.2807 | Test Accuracy: 87.33%\n",
      "Test Loss: 0.2807 | Test Accuracy: 87.33%\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
