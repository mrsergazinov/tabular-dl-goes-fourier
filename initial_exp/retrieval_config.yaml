dataset:
  name: "adult"
  version: 2
  test_size: 0.2
  random_state: 42
  batch_size: 32
  cat_encode: onehot
  all_num: False

model:
  dim: 128
  dropout: 0.1
  n_frequencies: 77
  frequency_scale: 0.04431360576139521
  d_embedding: 34
  lite: true
  temperature: 1.0
  sample_rate: 0.1
  use_llama: False
  llama_model_name: "meta-llama/Llama-3.2-1B" # "meta-llama/Llama-3.2-1B"
  start_layer: 0
  end_layer: 8

training:
  learning_rate: 0.001
  weight_decay: 0.0002
  epochs: 10
  model_path: "model.pt"

# without: 86.96
# llama 3.2 1B
#   layers 0-1: 86.52
#   removed top projection: 
#     layers 0-1: 86.61
#     layers 0-8: 86.74
# mixed precision:
#    llama 3 8B
#       removed top projection:
#          layers 0-8: 86.85

# subsample 10000
#   encoding categorical with LLM: 85.46
#   encoding categorical as label: 84.80
#   encoding categorical as onehot: 85.05
#   no PLR encoding for categorical
#      encoding categorical as label: 84.65
#      encoding categorical as onehot: 85.35
#      encoding categorical with LLM: 81.95
# full 
#  encoding categorical with LLM: 87.43 
#    file_name: model_llm_encode_1.pt
#  no PLR encoding for categorical + onehot: 87.33
#    file_name: model_onehot_1.pt
