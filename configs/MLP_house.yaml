MLP:
  d_layers: 203
  dropout: 0.175771593859265
  num_layers: 4
training:
  batch_size: 43
  epochs: 10
  learning_rate: 0.0004786080358753829
  weight_decay: 0.003881501661354007
