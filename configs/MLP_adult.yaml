MLP:
  d_layers: 94
  dropout: 0.10130620225778844
  num_layers: 4
training:
  batch_size: 36
  epochs: 10
  learning_rate: 0.0008804035234673775
  weight_decay: 0.0042605009937306
