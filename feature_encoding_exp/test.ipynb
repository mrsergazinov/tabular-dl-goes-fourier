{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import typing as ty\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "os.chdir('/home/mrsergazinov/TabLLM/feature_encoding_exp/')\n",
    "from base_models.mlp import MLP\n",
    "from base_models.tabTransformer import TabTransformer\n",
    "from base_models.modernNCA import ModernNCA\n",
    "from encoders.numEncoders import FourierFeatures\n",
    "\n",
    "MODELS = {\n",
    "    'MLP': MLP,\n",
    "    'TabTransformer': TabTransformer,\n",
    "    'ModernNCA': ModernNCA\n",
    "}\n",
    "ENCODERS = {\n",
    "    'FourierFeatures': FourierFeatures\n",
    "}\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# load yaml config from confgs/adult.yaml\n",
    "with open('configs/adult.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [0/611] | Loss: 0.5349\n",
      "Iteration [50/611] | Loss: 0.2847\n",
      "Iteration [100/611] | Loss: 0.2473\n",
      "Iteration [150/611] | Loss: 0.4159\n",
      "Iteration [200/611] | Loss: 0.4194\n",
      "Iteration [250/611] | Loss: 0.2066\n",
      "Iteration [300/611] | Loss: 0.2435\n",
      "Iteration [350/611] | Loss: 0.2743\n",
      "Iteration [400/611] | Loss: 0.3481\n",
      "Iteration [450/611] | Loss: 0.1847\n",
      "Iteration [500/611] | Loss: 0.2861\n",
      "Iteration [550/611] | Loss: 0.2399\n",
      "Iteration [600/611] | Loss: 0.2253\n",
      "Epoch [1/10] | Loss: 0.3156 | Accuracy: 0.8550 | Time: 7.87s\n",
      "Iteration [0/611] | Loss: 0.2487\n",
      "Iteration [50/611] | Loss: 0.4153\n",
      "Iteration [100/611] | Loss: 0.4307\n",
      "Iteration [150/611] | Loss: 0.2277\n",
      "Iteration [200/611] | Loss: 0.2761\n",
      "Iteration [250/611] | Loss: 0.3136\n",
      "Iteration [300/611] | Loss: 0.2167\n",
      "Iteration [350/611] | Loss: 0.2720\n",
      "Iteration [400/611] | Loss: 0.2885\n",
      "Iteration [450/611] | Loss: 0.2219\n",
      "Iteration [500/611] | Loss: 0.4573\n",
      "Iteration [550/611] | Loss: 0.2378\n",
      "Iteration [600/611] | Loss: 0.2590\n",
      "Epoch [2/10] | Loss: 0.2896 | Accuracy: 0.8647 | Time: 6.88s\n",
      "Iteration [0/611] | Loss: 0.3029\n",
      "Iteration [50/611] | Loss: 0.2775\n",
      "Iteration [100/611] | Loss: 0.3507\n",
      "Iteration [150/611] | Loss: 0.2001\n",
      "Iteration [200/611] | Loss: 0.2040\n",
      "Iteration [250/611] | Loss: 0.3015\n",
      "Iteration [300/611] | Loss: 0.3109\n",
      "Iteration [350/611] | Loss: 0.2871\n",
      "Iteration [400/611] | Loss: 0.2445\n",
      "Iteration [450/611] | Loss: 0.2716\n",
      "Iteration [500/611] | Loss: 0.2550\n",
      "Iteration [550/611] | Loss: 0.3178\n",
      "Iteration [600/611] | Loss: 0.3196\n",
      "Epoch [3/10] | Loss: 0.2851 | Accuracy: 0.8669 | Time: 7.10s\n",
      "Iteration [0/611] | Loss: 0.2140\n",
      "Iteration [50/611] | Loss: 0.2562\n",
      "Iteration [100/611] | Loss: 0.2633\n",
      "Iteration [150/611] | Loss: 0.3051\n",
      "Iteration [200/611] | Loss: 0.2256\n",
      "Iteration [250/611] | Loss: 0.3717\n",
      "Iteration [300/611] | Loss: 0.2565\n",
      "Iteration [350/611] | Loss: 0.2308\n",
      "Iteration [400/611] | Loss: 0.3323\n",
      "Iteration [450/611] | Loss: 0.3586\n",
      "Iteration [500/611] | Loss: 0.3024\n",
      "Iteration [550/611] | Loss: 0.2570\n",
      "Iteration [600/611] | Loss: 0.2596\n",
      "Epoch [4/10] | Loss: 0.2808 | Accuracy: 0.8673 | Time: 6.88s\n",
      "Iteration [0/611] | Loss: 0.3148\n",
      "Iteration [50/611] | Loss: 0.2199\n",
      "Iteration [100/611] | Loss: 0.1563\n",
      "Iteration [150/611] | Loss: 0.2035\n",
      "Iteration [200/611] | Loss: 0.2603\n",
      "Iteration [250/611] | Loss: 0.3693\n",
      "Iteration [300/611] | Loss: 0.1361\n",
      "Iteration [350/611] | Loss: 0.3313\n",
      "Iteration [400/611] | Loss: 0.2304\n",
      "Iteration [450/611] | Loss: 0.2037\n",
      "Iteration [500/611] | Loss: 0.2845\n",
      "Iteration [550/611] | Loss: 0.3588\n",
      "Iteration [600/611] | Loss: 0.2900\n",
      "Epoch [5/10] | Loss: 0.2784 | Accuracy: 0.8706 | Time: 6.89s\n",
      "Iteration [0/611] | Loss: 0.3036\n",
      "Iteration [50/611] | Loss: 0.2093\n",
      "Iteration [100/611] | Loss: 0.3009\n",
      "Iteration [150/611] | Loss: 0.2284\n",
      "Iteration [200/611] | Loss: 0.2593\n",
      "Iteration [250/611] | Loss: 0.2752\n",
      "Iteration [300/611] | Loss: 0.2285\n",
      "Iteration [350/611] | Loss: 0.1961\n",
      "Iteration [400/611] | Loss: 0.2250\n",
      "Iteration [450/611] | Loss: 0.3483\n",
      "Iteration [500/611] | Loss: 0.1556\n",
      "Iteration [550/611] | Loss: 0.3569\n",
      "Iteration [600/611] | Loss: 0.2360\n",
      "Epoch [6/10] | Loss: 0.2763 | Accuracy: 0.8710 | Time: 7.12s\n",
      "Iteration [0/611] | Loss: 0.2812\n",
      "Iteration [50/611] | Loss: 0.2700\n",
      "Iteration [100/611] | Loss: 0.2458\n",
      "Iteration [150/611] | Loss: 0.2293\n",
      "Iteration [200/611] | Loss: 0.2324\n",
      "Iteration [250/611] | Loss: 0.3308\n",
      "Iteration [300/611] | Loss: 0.2711\n",
      "Iteration [350/611] | Loss: 0.3066\n",
      "Iteration [400/611] | Loss: 0.2192\n",
      "Iteration [450/611] | Loss: 0.1691\n",
      "Iteration [500/611] | Loss: 0.4017\n",
      "Iteration [550/611] | Loss: 0.2557\n",
      "Iteration [600/611] | Loss: 0.1768\n",
      "Epoch [7/10] | Loss: 0.2754 | Accuracy: 0.8709 | Time: 6.97s\n",
      "Iteration [0/611] | Loss: 0.3159\n",
      "Iteration [50/611] | Loss: 0.3340\n",
      "Iteration [100/611] | Loss: 0.2355\n",
      "Iteration [150/611] | Loss: 0.3082\n",
      "Iteration [200/611] | Loss: 0.1943\n",
      "Iteration [250/611] | Loss: 0.2387\n",
      "Iteration [300/611] | Loss: 0.1613\n",
      "Iteration [350/611] | Loss: 0.3933\n",
      "Iteration [400/611] | Loss: 0.1935\n",
      "Iteration [450/611] | Loss: 0.3883\n",
      "Iteration [500/611] | Loss: 0.1440\n",
      "Iteration [550/611] | Loss: 0.1788\n",
      "Iteration [600/611] | Loss: 0.1925\n",
      "Epoch [8/10] | Loss: 0.2738 | Accuracy: 0.8728 | Time: 7.13s\n",
      "Iteration [0/611] | Loss: 0.1554\n",
      "Iteration [50/611] | Loss: 0.3472\n",
      "Iteration [100/611] | Loss: 0.2441\n",
      "Iteration [150/611] | Loss: 0.2250\n",
      "Iteration [200/611] | Loss: 0.2056\n",
      "Iteration [250/611] | Loss: 0.3324\n",
      "Iteration [300/611] | Loss: 0.2692\n",
      "Iteration [350/611] | Loss: 0.2253\n",
      "Iteration [400/611] | Loss: 0.1961\n",
      "Iteration [450/611] | Loss: 0.3322\n",
      "Iteration [500/611] | Loss: 0.2656\n",
      "Iteration [550/611] | Loss: 0.4417\n",
      "Iteration [600/611] | Loss: 0.3127\n",
      "Epoch [9/10] | Loss: 0.2719 | Accuracy: 0.8715 | Time: 7.31s\n",
      "Iteration [0/611] | Loss: 0.3031\n",
      "Iteration [50/611] | Loss: 0.1448\n",
      "Iteration [100/611] | Loss: 0.2358\n",
      "Iteration [150/611] | Loss: 0.2261\n",
      "Iteration [200/611] | Loss: 0.2152\n",
      "Iteration [250/611] | Loss: 0.2868\n",
      "Iteration [300/611] | Loss: 0.2399\n",
      "Iteration [350/611] | Loss: 0.3994\n",
      "Iteration [400/611] | Loss: 0.2540\n",
      "Iteration [450/611] | Loss: 0.3592\n",
      "Iteration [500/611] | Loss: 0.2132\n",
      "Iteration [550/611] | Loss: 0.3784\n",
      "Iteration [600/611] | Loss: 0.2419\n",
      "Epoch [10/10] | Loss: 0.2716 | Accuracy: 0.8725 | Time: 7.29s\n",
      "Evaluation Metric: 87.2351 | Accuracy: 0.8724\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'model_name': 'ModernNCA',\n",
    "    'num_encoder': 'FourierFeatures',\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_openml(\"adult\", version=2, as_frame=True)\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_columns = X.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Split the data into training and test sets before processing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=config['seed']\n",
    ")\n",
    "\n",
    "# Encode the target variable\n",
    "le_target = LabelEncoder()\n",
    "y_train = le_target.fit_transform(y_train)\n",
    "y_test = le_target.transform(y_test)\n",
    "\n",
    "# Process categorical columns\n",
    "d_in_cat = None\n",
    "if params['model_name'] == 'TabTransformer':\n",
    "    # encode with label encoding\n",
    "    d_in_cat = []\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        X_train[col] = le.fit_transform(X_train[col])\n",
    "        X_test[col] = le.transform(X_test[col])\n",
    "        d_in_cat.append(len(le.classes_))\n",
    "    X_train_cat = X_train[categorical_columns].copy()\n",
    "    X_test_cat = X_test[categorical_columns].copy()\n",
    "else:\n",
    "    X_train_cat = pd.get_dummies(X_train[categorical_columns], drop_first=True)\n",
    "    X_test_cat = pd.get_dummies(X_test[categorical_columns], drop_first=True)\n",
    "\n",
    "    # Align the test and train categorical features to prevent data leakage\n",
    "    X_train_cat, X_test_cat = X_train_cat.align(X_test_cat, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Scale numerical columns\n",
    "numerical_transformer = StandardScaler()\n",
    "X_train_num = numerical_transformer.fit_transform(X_train[numerical_columns])\n",
    "X_test_num = numerical_transformer.transform(X_test[numerical_columns])\n",
    "\n",
    "# Convert to tensors\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "X_train_num = torch.tensor(X_train_num, dtype=torch.float32)\n",
    "X_test_num = torch.tensor(X_test_num, dtype=torch.float32)\n",
    "X_train_cat = torch.tensor(X_train_cat.values, dtype=torch.float32)\n",
    "X_test_cat = torch.tensor(X_test_cat.values, dtype=torch.float32)\n",
    "\n",
    "if params['model_name'] == 'TabTransformer':\n",
    "    X_train_cat = X_train_cat.to(torch.long)\n",
    "    X_test_cat = X_test_cat.to(torch.long)\n",
    "\n",
    "# Determine input dimensions\n",
    "d_in_num = X_train_num.shape[1]\n",
    "d_in_cat = X_train_cat.shape[1] if d_in_cat is None else d_in_cat\n",
    "d_out = len(np.unique(y_train))\n",
    "\n",
    "# Define numerical feature encoder\n",
    "if params['num_encoder']:\n",
    "    num_encoder = ENCODERS[params['num_encoder']](\n",
    "        n_features=d_in_num,\n",
    "        **config[params['num_encoder']],\n",
    "    )\n",
    "else:\n",
    "    num_encoder = None\n",
    "\n",
    "# Define the model\n",
    "model = MODELS[params['model_name']](\n",
    "    d_in_num=d_in_num,\n",
    "    d_in_cat=d_in_cat,\n",
    "    d_out=d_out,\n",
    "    num_encoder=num_encoder,\n",
    "    **config[params['model_name']],\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss criterion\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Start training\n",
    "model.fit(\n",
    "    X_num_train=X_train_num,\n",
    "    X_cat_train=X_train_cat,\n",
    "    y_train=y_train,\n",
    "    criterion=loss_criterion,\n",
    "    **config['training'],\n",
    ")\n",
    "\n",
    "# Define the accuracy criterion function\n",
    "def accuracy_criterion(outputs: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct = (predicted == targets).sum().item()\n",
    "        accuracy = correct / targets.size(0)\n",
    "    return accuracy * 100\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "model.evaluate(\n",
    "    X_num_test=X_test_num,\n",
    "    X_cat_test=X_test_cat,\n",
    "    y_test=y_test,\n",
    "    criterion=accuracy_criterion,\n",
    "    batch_size=32,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
